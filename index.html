<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation Metrics</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Evaluation Metrics</h1>
            <h2>for Deep Learning Methods for 3D Facial Animation Synthesis</h2>
            <p>Alkiviadis Pavlou, Kazi Injamamul Haque, Zerrin Yumak</p>
            <p>Utrecht University</p>
            <div class="buttons">
                <a href="#" class="btn">Paper</a>
                <a href="#" class="btn">arXiv</a>
                <a href="#" class="btn">Code</a>
                <a href="#" class="btn">Demo</a>
            </div>
        </header>

        <main>
            <section id="abstract">
                <h3>Abstract</h3>
                <p>The recent advancements in the research field of Audio-Driven Facial Animations have provided both new state-of-the-art approaches and more topics to be discovered in depth. One of the main factors of any new method is the ability to evaluate its achievements accurately and representatively. In Audio-Driven Facial Animations, this is achieved using evaluation metrics that either objectively or subjectively compare state-of-the-art models. As expected from an early-stage research field, the evaluation section has been following the typical path focusing on simple metrics in most cases. This study aims to clarify the process for future researchers through an in-depth analysis of evaluation metrics. Another important aspect of this study is the understanding of the different approaches deterministic and non-deterministic models need in terms of the evaluation of the results. Apprehending the nature of facial animations will also be examined by exploring ground truth datasets in various ways.</p>
            </section>

            <section id="methodology">
                <h3>Methodology</h3>
                <h4>Nature of Facial Animations</h4>
                <p>Multiple state-of-the-art datasets are tested using their Ground Truth dataset aiming to analyse the non-determinism that exists in subjects and emotion categories. Using dimensionality reduction techniques such as t-SNE and PCA we evaluated the correlation between same-subject and same-emotion sequences that are performed in datasets such as BIWI, Multiface, 3DMEAD and BEAT. During this process the results proved the existence of non-determinism across sequences that are performed by the same actor with correlation existing between them. Multiple different experiments were used to analyse the results concluding to the nature of Facial Animations.</p>

                <h4>Metrics Inventory</h4>
                <p>The study collected the most insightful evaluation metrics existing for 3D Facial Animations and tested them on trained state-of-the-art models providing a clear overview of the results for each metric. FaceXHuBERT, FaceFormer, FaceDiffuser, CodeTalker, CodeTalker-ND and ProbTalk3D were all locally trained on BIWI, Multiface and 3DMEAD aiming to provide transparent results for better evaluation of the metrics that were used. For each of the models on each dataset we calculated the objective metrics: LVE, FDD, MVE, Diversity, MEE and CE and also conducted a Perception Study to calculate the Subjective Metrics of Realism and Lip-Sync. In-depth analysis of the results was conducted aiming to identify the pros and cons of each metric alongside the variables that affect them.</p>
            </section>

            <section id="bibtex">
                <h3>BibTeX</h3>
                <pre>
     later
                </pre>
            </section>
        </main>

    </div>
</body>
</html>
